---
title: "HW3_Markdown"
author: "Noah Kreski"
date: "October 12, 2018"
output: github_document
---
``` {r setup, include = FALSE}
library(tidyverse)
# install.packages("devtools")
devtools::install_github("p8105/p8105.datasets")
#loading all extra libraries needed
library(p8105.datasets)
```

#Problem One

```{r BRFSS}
#This imports the data and cleans the names.
BRFSS_Cleaned = janitor::clean_names(brfss_smart2010)%>%
                #The next two lines focus on overall health and entries with one of five relevant response levels.
                filter(topic == "Overall Health")%>%
                filter(response %in% c("Excellent", "Very good", "Good", "Fair", "Poor"))%>%
                #This creates an ordered factor of response levels.
                mutate(response = (factor (response, levels = c("Excellent", "Very good", "Good", "Fair", "Poor"))))
                
```

###In 2002, which states were observed at 7 locations?

```{r 2002 locations}
#This gets the 2002 entries.
filter(BRFSS_Cleaned, year == 2002)%>%
#This includes a single datum for each location
distinct(locationdesc,.keep_all = TRUE)%>%
#These two lines tell us how many locations exist by state
group_by(locationabbr)%>%
summarize(n = n())
```

The states observed at 7 locations in 2002 are Connecticut, Florida, and North Carolina

###Make a “spaghetti plot” that shows the number of locations in each state from 2002 to 2010.

```{r spaghetti}

BRFSS_Spaghetti = BRFSS_Cleaned%>%
                  #This includes all unique locations by year
                  distinct(year,locationdesc,.keep_all = TRUE)%>%
                  #These two lines tell us how many locations exist by state and year
                  group_by(year,locationabbr)%>%
                  summarize(n = n())
ggplot(data = BRFSS_Spaghetti, aes(x=year, y=n, group=locationabbr, color = locationabbr)) + geom_line()
```

This spaghetti plot tracks the number of observations by year and state.

###Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.
```{r NY Excellent}
#Th sese two steps will isolate the years needed and limit data to excellent responses across NY
filter(BRFSS_Cleaned, year %in% c(2002, 2006, 2010))%>%
filter(response == "Excellent", locationabbr == "NY")%>%
#This sets up the data for exploratory summarization, and the production of a mean and standard deviation
group_by(year)%>%
summarize(mean_excellent = mean(data_value), sd_excellent = sd(data_value))

```

This table shows the average proportion and standard deviation of Excellent responses in New York in 2002, 2006, and 2010.
###For each year and state, compute the average proportion in each response category (taking the average across locations in a state). Make a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time

```{r Five Panel, Warning = FALSE}
#Getting a data set with a proportion for each response level by state and year
BRFSS_Five= BRFSS_Cleaned%>%
            group_by(year, locationabbr, response)%>%
            summarize(mean_response = mean(data_value))%>%
            group_by(year, response)
#I am setting up a 5 plot panel
par(mfrow=c(2,3))
par(mar=c(3,2,1,2))
#These boxplots graph each response level over time.
boxplot(mean_response~year,filter(BRFSS_Five, response == "Excellent"), main = "Excellent")
boxplot(mean_response~year,filter(BRFSS_Five, response == "Very good"), main = "Very Good")
boxplot(mean_response~year,filter(BRFSS_Five, response == "Good"), main = "Good")
boxplot(mean_response~year,filter(BRFSS_Five, response == "Fair"), main = "Fair")
boxplot(mean_response~year,filter(BRFSS_Five, response == "Poor"), main = "Poor")
```

#Problem Two

```{r Instacart, include = FALSE}
instacart
```

###Write a short description of the dataset, noting the size and structure of the data, describing some key variables, and giving illstrative examples of observations

This data, with 1,384,617 rows and 15 columns, articulates information about grocery shopping behavior. Key variables include aisle, which describes the location of an item, and product_id, which gives a numeric code for a purchased product. For instance, order_id 1 included items such as Bulgarian Yogurt and Organic 4% Milk Fat Whole Milk, both in the department 'dairy eggs'. Additionally, I know the hour of day during this order was 10 am, that it has been 9 days since the last order, and that yogurt was added to the cart first.

###How many aisles are there, and which aisles are the most items ordered from?
```{r aisles}
#These three lines generate the number of observations for every aisle, and by extension, includes every aisle as a row. 
instacart%>%
group_by(aisle)%>%
summarize(obs=n())

#This code matches the above, but then isolates the 5 most common/popular aisles.
instacart%>%
group_by(aisle)%>%
summarize(obs=n())%>%
top_n(n=5)
```
This summarization code provides a summary with 134 rows, one for each aisle, so there are 134 aisles. The 5 most popular aisles are fresh fruits, fresh vegetables, packaged cheese, packages vegetables & fruits, and yogurt, the number one aisle being fresh vegetables.

###Make a plot that shows the number of items ordered in each aisle. Order aisles sensibly, and organize your plot so others can read it.
```{r aisles plot}
#These three lines create a data set with the number of items per aisle, ready for plotting
instacart_summary_data = instacart%>%
                         group_by(aisle_id)%>%
                         summarize(obs=n())
#These lines generate the plot with aisle in numerical order, and appropriate labels
ggplot(instacart_summary_data, aes(x = aisle_id, y = obs)) + 
  geom_point() + labs(title = "Items Ordered per aisle", x = "Aisle", y = "Orders")
```

###Make a table showing the most popular item in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”.

```{r popular items}
#These two steps isolate the aisles of interest and group by aisle and product name for analyses
filter(instacart, aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits"))%>%
group_by(aisle, product_name)%>%
#These remaining steps generate a sample size per product, groups by aisle, and generates the most common product in each category
count(product_name)%>%
group_by(aisle)%>%
top_n(1)
```
The most common product purchased in the aisles baking ingredients, dog food care, and packaged vegetables fruits are as follows: Light Brown Sugar, Snack Sticks Chicken & Rice Recipe Dog Treats, and Organic Baby Spinach.

###Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).

```{r pink lady}
#These two lines limit the data to the two products of interest, and then group by product and day of week
filter(instacart, product_name %in% c("Pink Lady Apples", "Coffee Ice Cream"))%>%
group_by(product_name, order_dow)%>%
#These two lines generate a mean hour of the day for each day of the week and product, before creating a more readable 2x7 table
summarize(mean_hour = mean(order_hour_of_day))%>%
spread(key=order_dow, value=mean_hour)
```
The above table shows the mean hour of the day for each day of the week, per product. While we can assume 0 is Sunday here, without this explicit information, this table remains an accurate representation of the necessary data.

#Problem Three

###The goal is to do some exploration of this dataset. To that end, write a short description of the dataset, noting the size and structure of the data, describing some key variables, and indicating the extent to which missing data is an issue. 
```{r noaa, eval = FALSE}
#I am checking the general structure of the dataset
ny_noaa
#I am seeing the distribution of prcp, and how much is missing. The statements below are the same goal for each variable
ny_noaa%>%
count(prcp)

ny_noaa%>%
count(snow)

ny_noaa%>%
count(snwd)
  
ny_noaa%>%
count(tmax)
  
ny_noaa%>%
count(tmin)
```

This data set, which contains 7 columns and 2,595,176 rows, features a few critical variables. These include date and id, which gives the location and timing of observations, and data for weather information like temperature or snowfall. Missing data is a huge issue, with at least 100,000 NA values for each values, and over 1,000,000 for each temperature variable.

###Do some data cleaning. Create separate variables for year, month, and day. Ensure observations for temperature, precipitation, and snowfall are given in reasonable units. For snowfall, what are the most commonly observed values? Why?

```{r Noaa clean}
ny_noaa_cleaned = ny_noaa%>%
                  #This separates the date variable into relevant categories.
                  separate(date, c("Year", "Month", "Day"), sep = "-")%>%
                  #This converts from tenths of mm to full millimeters, and from tenths of degrees Celsius to full degrees.
                  mutate(prcp = prcp/10, tmax = as.numeric(tmax)/10, tmin = as.numeric(tmin)/10)
#This Checks the most common snowfall values
ny_noaa_cleaned%>%
count(snow)%>%
top_n(2)
```

The most common answers for amount of snowfall are 0, as many places don't get any snow, and NA, as many places just do not document this.

###Make a two-panel plot showing the average max temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?

```{r tmax}
maxt= ny_noaa_cleaned%>%
      filter(Month %in% c("01", "07"))%>%
      group_by(Year, Month)%>%
      summarise(tmean = mean(tmax, na.rm = TRUE))
par(mfrow = c(1,2))
      plot(filter(maxt, Month == "01")$Year, filter(maxt, Month == "01")$tmean, xlab = "Year", ylab = "tmax", main = "January")
      plot(filter(maxt, Month == "07")$Year, filter(maxt, Month == "07")$tmean, xlab = "Year", ylab = "tmax", main = "July")

```

The above plots show the change in mean maximum temperature over the years for January and July. Any patterns are difficult to discern from this visualization alone, and further analysis may be necessary. Outliers, such as a uniquely low 1994 January temperature or uniquely low 2009 July temperature, do exist.

###Make a two-panel plot showing (i) tmax vs tmin for the full dataset (note that a scatterplot may not be the best option); and (ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.

```{r maxmin, warning = FALSE}
require(hexbin)
library(grid)
library(gridExtra)
plotX <- ggplot(ny_noaa_cleaned, aes(x = tmax, y = tmin)) + geom_hex()
plotY <-ggplot(filter(ny_noaa_cleaned, snow > 0 & snow < 100), aes(x = Year, y = snow)) + geom_boxplot()+scale_x_discrete(breaks = c("1981", "1990", "2000", "2010"))

grid.arrange(plotX, plotY)


```

The above visuals show the distribution of tmax vs tmin, with lighter blues showing higher frequency, and boxplots of snowfall by year.